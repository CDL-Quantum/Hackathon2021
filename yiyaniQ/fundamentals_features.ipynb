{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff0ec8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"agg\")    # must select backend before importing pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "292f03cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimod\n",
    "from dwave.system import DWaveCliqueSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d83ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MI calculations\n",
    "def prob(dataset):\n",
    "    \"\"\"Joint probability distribution P(X) for the given data.\"\"\"\n",
    "\n",
    "    # bin by the number of different values per feature\n",
    "    num_rows, num_columns = dataset.shape\n",
    "    bins = [len(np.unique(dataset[:, ci])) for ci in range(num_columns)]\n",
    "\n",
    "    prob, _ = np.histogramdd(dataset, bins)\n",
    "    return prob / np.sum(prob)\n",
    "\n",
    "\n",
    "def shannon_entropy(p):\n",
    "    \"\"\"Shannon entropy H(X) is the negative sum of P(X)log(P(X)) for probability\n",
    "    distribution P(X).\n",
    "    \"\"\"\n",
    "    p = p.flatten()\n",
    "    return -sum(pi*np.log2(pi) for pi in p if pi)\n",
    "\n",
    "\n",
    "def conditional_shannon_entropy(p, *conditional_indices):\n",
    "    \"\"\"Conditional Shannon entropy H(X|Y) = H(X,Y) - H(Y).\"\"\"\n",
    "\n",
    "    # Sanity check on validity of conditional_indices.  In particular,\n",
    "    # try to trap issues in which dimensions have been removed from\n",
    "    # probability table through marginalization, but\n",
    "    # conditional_indices were not updated accordingly.\n",
    "    assert(all(ci < p.ndim for ci in conditional_indices))\n",
    "\n",
    "    axis = tuple(i for i in np.arange(len(p.shape))\n",
    "                 if i not in conditional_indices)\n",
    "\n",
    "    return shannon_entropy(p) - shannon_entropy(np.sum(p, axis=axis))\n",
    "\n",
    "\n",
    "def mutual_information(prob, j):\n",
    "    \"\"\"Mutual information between variables X and variable Y.\n",
    "\n",
    "    Calculated as I(X; Y) = H(X) - H(X|Y).\"\"\"\n",
    "\n",
    "    return (shannon_entropy(np.sum(prob, axis=j))\n",
    "            - conditional_shannon_entropy(prob, j))\n",
    "\n",
    "\n",
    "def conditional_mutual_information(p, j, *conditional_indices):\n",
    "    \"\"\"Mutual information between variables X and variable Y conditional on variable Z.\n",
    "\n",
    "    Calculated as I(X;Y|Z) = H(X|Z) - H(X|Y,Z)\"\"\"\n",
    "\n",
    "    # Compute an updated version of the conditional indices for use\n",
    "    # when the probability table is marginalized over dimension j.\n",
    "    # This marginalization removes one dimension, so any conditional\n",
    "    # indices pointing to dimensions after this one must be adjusted\n",
    "    # accordingly.\n",
    "    marginal_conditional_indices = [i-1 if i > j else i for i in conditional_indices]\n",
    "\n",
    "    return (conditional_shannon_entropy(np.sum(p, axis=j), *marginal_conditional_indices)\n",
    "            - conditional_shannon_entropy(p, j, *conditional_indices))\n",
    "\n",
    "\n",
    "def maximum_energy_delta(bqm):\n",
    "    \"\"\"Compute conservative bound on maximum change in energy when flipping a single variable\"\"\"\n",
    "    return max(abs(bqm.get_linear(i))\n",
    "               + sum(abs(bqm.get_quadratic(i,j))\n",
    "                     for j in bqm.iter_neighbors(i))\n",
    "               for i in bqm.iter_variables())\n",
    "\n",
    "\n",
    "def mutual_information_bqm(dataset, features, target):\n",
    "    \"\"\"Build a BQM that maximizes MI between survival and a subset of features\"\"\"\n",
    "    variables = ((feature, -mutual_information(prob(dataset[[target, feature]].values), 1))\n",
    "                 for feature in features)\n",
    "    interactions = ((f0, f1, -conditional_mutual_information(prob(dataset[[target, f0, f1]].values), 1, 2))\n",
    "                    for f0, f1 in itertools.permutations(features, 2))\n",
    "    return dimod.BinaryQuadraticModel(variables, interactions, 0, dimod.BINARY)\n",
    "\n",
    "\n",
    "def add_combination_penalty(bqm, k, penalty):\n",
    "    \"\"\"Create a new BQM with an additional penalty biased towards k-combinations\"\"\"\n",
    "    kbqm = dimod.generators.combinations(bqm.variables, k, strength=penalty)\n",
    "    kbqm.update(bqm)\n",
    "    return kbqm\n",
    "\n",
    "\n",
    "def mutual_information_feature_selection(dataset, features, target, num_reads=5000):\n",
    "    \"\"\"Run the MIFS algorithm on a QPU solver\"\"\"\n",
    "    \n",
    "    # Set up a QPU sampler that embeds to a fully-connected graph of all the variables\n",
    "    sampler = DWaveCliqueSampler()\n",
    "\n",
    "    # For each number of features, k, penalize selection of fewer or more features\n",
    "    selected_features = np.zeros((len(features), len(features)))\n",
    "\n",
    "    bqm = mutual_information_bqm(dataset, features, target)\n",
    "\n",
    "    # This ensures that the soltion will satisfy the constraints.\n",
    "    penalty = maximum_energy_delta(bqm)\n",
    "\n",
    "    for k in range(1, len(features) + 1):\n",
    "        kbqm = add_combination_penalty(bqm, k, penalty)\n",
    "        sample = sampler.sample(kbqm,\n",
    "                                label='Example - MI Feature Selection',\n",
    "                                num_reads=num_reads).first.sample\n",
    "        for fi, f in enumerate(features):\n",
    "            selected_features[k-1, fi] = sample[f]\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "070d580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_demo1(dataset, target):\n",
    "    \"\"\"Compute MIFS for each value of k and visualize results\"\"\"\n",
    "\n",
    "    # Rank the MI between survival and every other variable\n",
    "    scores = {feature: mutual_information(prob(dataset[[target, feature]].values), 0)\n",
    "              for feature in set(dataset.columns) - {target}}\n",
    "\n",
    "    labels, values = zip(*sorted(scores.items(), key=lambda pair: pair[1], reverse=True))\n",
    "    return labels, values, scores\n",
    "    # Plot the MI between survival and every other variable\n",
    "    # The Titanic dataset provides a familiar, intuitive example available in the public\n",
    "    # domain. In itself, however, it is not a good fit for solving by sampling. Run naively on\n",
    "    # this dataset, it finds numerous good solutions but is unlikely to find the exact optimal solution.\n",
    "    # There are many techniques for reformulating problems for the D-Wave system that can\n",
    "    # improve performance on various metrics, some of which can help narrow down good solutions\n",
    "    # to closer approach an optimal solution.\n",
    "    # This demo solves the problem for just the highest-scoring features.\n",
    "def run_demo2(dataset, scores,target):\n",
    "    # Select 8 features with the top MI ranking found above.\n",
    "    keep = 8\n",
    "    \n",
    "    sorted_scores = sorted(scores.items(), key=lambda pair: pair[1], reverse=True)\n",
    "    dataset = dataset[[column[0] for column in sorted_scores[0:keep]] + [target]]\n",
    "    features = sorted(list(set(dataset.columns) - {target}))\n",
    "    selected_features = mutual_information_feature_selection(dataset, features, target)\n",
    "    return features, selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d04659ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_1(labels, values, target):\n",
    "    plt.figure()\n",
    "    #ax1 = plt.subplot(1, 0, 0)\n",
    "    \n",
    "    plt.gcf().subplots_adjust(bottom=0.5)\n",
    "    plt.bar(np.arange(len(labels)), values)\n",
    "    plt.title(\"Mutual Information\")\n",
    "    ylabel_='MI Between ' + target +  ' and Feature'\n",
    "    plt.ylabel(ylabel_)\n",
    "    plt.xticks(np.arange(len(labels)), labels, rotation=90, fontsize=11)\n",
    "    name=\"plots_MI_\" + target+ \".png\"\n",
    "    plt.savefig(name)\n",
    "\n",
    "def plotting2(selected_features,features,target):    \n",
    "    # Plot the best feature set per number of selected features\n",
    "    plt.figure()\n",
    "    plt.gcf().subplots_adjust(bottom=0.5)\n",
    "    ax2 = plt.subplot(1, 1, 1)\n",
    "    ax2.set_title(\"Best Feature Selection\")\n",
    "    ax2.set_ylabel('Number of Selected Features')\n",
    "    ax2.set_xticks(np.arange(len(features)))\n",
    "    ax2.set_xticklabels(features, rotation=90)\n",
    "    ax2.set_yticks(np.arange(len(features)))\n",
    "    ax2.set_yticklabels(np.arange(1, len(features)+1))\n",
    "    # Set a grid on minor ticks\n",
    "    ax2.set_xticks(np.arange(-0.5, len(features)), minor=True)\n",
    "    ax2.set_yticks(np.arange(-0.5, len(features)), minor=True)\n",
    "    ax2.grid(which='minor', color='black')\n",
    "    ax2.imshow(selected_features, cmap=colors.ListedColormap(['white', 'red']))\n",
    "    name=\"plots_slecetedFeatures_\" + target+ \".png\"\n",
    "    plt.savefig(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d7c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_feats(df,target):\n",
    "    labels, values, scores=run_demo1(df, target)\n",
    "    features, selected_features=run_demo2(df, scores, target)\n",
    "    plotting_1(labels, values, target)\n",
    "    plotting2(selected_features, features, target)\n",
    "    return features\n",
    "\n",
    "def calc_col_score(df,col,df_lookup):\n",
    "    look=df_lookup[2].loc[col]\n",
    "    score_col=0.0\n",
    "    num=len(df[col])-1\n",
    "    for i in range(num):\n",
    "        x=df[col].loc[i]-df[col].loc[i+1]\n",
    "        if x==abs(x) and look == 'a' : score_col += 1\n",
    "        if x==-abs(x) and look == 'd' : score_col += 1\n",
    "    return score_col/(num+1e-7) \n",
    "\n",
    "def scoring(features,df0,cutoff_date):\n",
    "    feats=['ticker','calendardate']+features\n",
    "    df=df0[pd.to_datetime(df0['calendardate']) > pd.to_datetime(cutoff_date, format='%Y%m%d', errors='ignore')]\n",
    "    df_scoring=df[feats]\n",
    "    \n",
    "    df_lookup0=pd.read_csv('light_featureDef.csv',header=None)\n",
    "    df_lookup=df_lookup0.set_index(df_lookup0[0]).drop([0],axis=1)\n",
    "    \n",
    "    df_finScore=pd.DataFrame(index=df_scoring['ticker'].unique(),columns=['score'])\n",
    "    \n",
    "    for f in df_scoring.groupby('ticker'):\n",
    "        score=0\n",
    "        df_work=f[1].reset_index()\n",
    "        for col in features:\n",
    "            score+=calc_col_score(df_work,col,df_lookup)\n",
    "        score/=len(features)    \n",
    "        df_finScore['score'].loc[df_work['ticker'].loc[0]]=score\n",
    "    \n",
    "    return df_finScore.sort_values('score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c87b9dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_portfolio(cutoff_date,target,file_name): #cutoff_date is a string of format yyyymmdd , target could be price or log_ret\n",
    "    dataset0= pd.read_csv('data/fundamentals_normalized.csv')\n",
    "    if target == 'price':\n",
    "        drop_list=['ticker','ticker.1','Unnamed: 1','index','None','dimension','calendardate','datekey','reportperiod' ,'lastupdated','log_ret']\n",
    "    else:\n",
    "        drop_list=['ticker','ticker.1','Unnamed: 1','index','None','dimension','calendardate','datekey','reportperiod' ,'lastupdated','price']\n",
    "            \n",
    "    df_target=dataset0[pd.to_datetime(dataset0['calendardate']) > pd.to_datetime(cutoff_date, format='%Y%m%d', errors='ignore')].dropna().drop(drop_list, axis = 1)\n",
    "    features=select_feats(df_target,target)\n",
    "    df_score=scoring(features,dataset0,cutoff_date)\n",
    "    df_score.to_csv(file_name, sep=\",\")\n",
    "    \n",
    "    return df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2eea4148",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = generate_portfolio('20201031','price', 'data/scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f1955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
