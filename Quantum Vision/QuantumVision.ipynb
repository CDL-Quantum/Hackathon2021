{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0660d849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: \"AMS\" } }});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: \"AMS\" } }});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34bb108",
   "metadata": {},
   "source": [
    "# Maximum Entropy Principle for inference methods\n",
    "\n",
    "The inference method based on the maximum entropy principle (**MaxEnt principle**) \n",
    "asserts that the most suitable probability distribution compatible with a given set of constraints is the one with the largest entropy [Jaynes1957a, Jaynes1957b].\n",
    "This method is considered as a powerful estimation technique in a wide range of probabilistic models since it brings a solution to the **universal problem** of trying to stract information from partial or incomplete data --which is usually what we have to work with--. That is the reason why it finds applications in various fields of research, beyond statistical physics, as biology [De Martino2018] and ecology [Tang2021], being also usefull for analyzing and understanding complex social or economic systems [Golan1997,Scharfenaker2020], and make financial predictions [Benedetto2015]. Problems arising from these systems are characterized by having many degrees of freedom and non-trivial interaction patterns between individual subsystems. Hence it must be dealt with\n",
    "inductive inference problems due to the insufficient amount of experimental data and the incomplete nature of the information that can be extracted from them.  \n",
    "\n",
    "Moreover, the MaxEnt principle has been proven to be useful for a reasonable estimation of quantum states from incomplete data [Buzek2000,Goncalves2013,Gupta2021], where the amount of experimental resourses and time consuming make quantum tomography impractical even for an intermediate number of qubits, and therefore, approaches to validate quantum processing on these quantum devices are needed.\n",
    "\n",
    "[De Martino2018] De Martino A, De Martino D. An introduction to the maximum entropy approach and its application to inference problems in biology. Heliyon. 2018 Apr 13;4(4):e00596. https://doi.org/10.1016/j.heliyon.2018.e00596 \n",
    "\n",
    "[Tang2021] Maximum Entropy Modeling to Predict the Impact of Climate Change on Pine Wilt Disease in China, Xinggang Tang, Yingdan Yuan, Xiangming Li and Jinchi Zhang, Front. Plant Sci., 23 April 2021. https://doi.org/10.3389/fpls.2021.652500.\n",
    "\n",
    "[Golan1997] A. Golan, G. Judge, and D. Miller, *Maximum Entropy\n",
    "Econometrics: Robust Estimation with Limited Data* (John Wiley and Sons, Chichester, United Kingdom,\n",
    "1997).\n",
    "\n",
    "[Scharfenaker2020] Scharfenaker, E., Yang, J. Maximum entropy economics. Eur. Phys. J. Spec. Top. 229, 1577–1590 (2020). https://doi.org/10.1140/epjst/e2020-000029-4\n",
    "\n",
    "[Benedetto2015] A maximum entropy method to assess the predictability of financial and commodity prices, F.Benedetto, G.Giunta, L.Mastroeni, Digital Signal Processing\n",
    "Volume 46, November 2015, Pages 19-31. https://doi.org/10.1016/j.dsp.2015.08.001\n",
    "\n",
    "[Jaynes1957a]  E. T. Jaynes, Information theory and statistical mechanics, Physical Review **106**, 620 (1957).\n",
    "\n",
    "[Jaynes1957b]  E. T. Jaynes, Information theory and statistical mechanics. II, Physical Review **108**, 171 (1957).\n",
    "\n",
    "[Buzek2000]  V. Buzek and G. Drobny, Quantum tomography via the maxent principle, Journal  of Modern  Optics47,  2823 (2000). https://doi.org/10.1080/09500340008232199\n",
    "\n",
    "[Goncalves2013]  D. Goncalves, C. Lavor, M. Gomes-Ruggiero, A. Cesario,R.  Vianna,  and  T.  Maciel,  Quantum  state  tomographywith incomplete data: Maximum entropy and variationalquantum tomography, Phys. Rev. A87, 052140 (2013). https://doi.org/10.1103/PhysRevA.87.052140\n",
    "\n",
    "\n",
    "[Gupta2021] Maximal Entropy Approach for Quantum State Tomography, Rishabh Gupta, Rongxin Xia, Raphael D. Levine, and Sabre Kais, PRX QUANTUM **2**, 010318 (2021). https://doi.org/10.1103/PRXQuantum.2.010318"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997ab6d2",
   "metadata": {},
   "source": [
    "\n",
    "# Mathematical problem\n",
    "\n",
    "Let $X$ be a random variable in a sample space $\\Omega = \\{x_1, \\ldots, x_k\\}$ with **unknown** probabilities \\\\(p_i=P(X=x_i), x_i ∈ \\Omega\\\\) and $\\sum_{i=1}^k p_i=1$. Mathematically, the MaxEnt formalism with \\\\(m\\\\) constraints on the expectations values $E[g_j]=\\alpha_j$ of functions $g_j(x_i)$, can be expressed as a constrained optimization problem \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{max}\\;S(X)\\;\\;\n",
    "\\mathrm{s.t.}\n",
    "\\sum_{i=1}^k p_i g_j(x_i)=\\alpha_j,  ~j=1,\\dots,m \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(1)\\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "were $S(X)$ is the entropy of the random variable $X$. Within the information theory, $S$ is usually taken as the Shannon entroypy and then the principle gives the less biased distribution, consistent with the available data. In such a case $S(X)= H \\equiv −k\n",
    "\\sum_{i=1}^k p_i \\mathrm{log}(p_i)$. The resulting maximum entropy probability is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "p_i =\n",
    "\\frac{1}\n",
    "{Z(λ_1, . . . , λ_m)}\n",
    "exp \\left[−λ_1 g_1(x_i) − · · · − λ_m g_m(x_i)\\right],\\;\\;\\;\\;\\;\\;\\;\\;\\;(2)\\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "with $Z(λ_1, . . . , λ_m) = \\sum_{i=1}^k exp [−λ_1g_1(x_i) − · · · − λ_mg_m(x_i)]$ and $λ_m$ is the Lagrangian multiplier for the $m$-th constraint given by the relation\n",
    "\\begin{equation}\\label{classical lagrange multipliers}\n",
    "\\alpha_{j}= \\frac{\\partial}{\\partial\\lambda_{j}}\\ln Z,\\quad    1\\leq j \\leq n.\\;\\;\\;\\;\\;\\;(3)\\nonumber\n",
    "\\end{equation}.\n",
    "\n",
    "Hence, to find the MaxEnt probability distribution is considered a hard task due to the nonlinearities in the reconstruction algorithm. In fact, the relations in Ec. (3) represents a system of nonlinear differential equations to be solve.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5603411a",
   "metadata": {},
   "source": [
    "# MaxEnt inference in Biology \n",
    "\n",
    "## Inference of gene interaction networks\n",
    "\n",
    "<img src=\"Ex1_bio.png\" height=5> \n",
    "\n",
    "**Fig. 1** Inference of gene interaction networks from empirical expression data. Figure extracted from \"Using the principle of entropy maximization to infer genetic interaction networks from gene expression patterns\", T.R. Lezon, J.R. Banavar, M. Cieplak, A. Maritan, N.V. Fedoroff, Proc. Natl. Acad. Sci. 103 (50) (2006) 19033–19038.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!--## Inference of genome-scale metabolic flux patterns from empirical growth rate distributions in bacteria\n",
    "\n",
    "<img src=\"Ex2_bio.png\">-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3786a7",
   "metadata": {},
   "source": [
    "# MaxEnt inference in Ecology\n",
    "\n",
    "## Predicting the distribution of pine species and the impact of climate change on forest diseases\n",
    "\n",
    "<img src=\"Ex_Clima.png\">\n",
    "\n",
    "**Fig. 2** Habitat suitability maps showing the ocurrence of *P. desinflora* by 2050 and 2070 under two distinct climate change scenarios in China.  Figure extracted from \"Maximum Entropy Modeling to Predict the Impact of Climate Change on Pine Wilt Disease in China\", Xinggang Tang, Yingdan Yuan, Xiangming Li and Jinchi Zhang, Front. Plant Sci., 23 April 2021.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c13f6",
   "metadata": {},
   "source": [
    "# Solving MaxEnt as a QUBO problem\n",
    "\n",
    "The Quadratic Unconstrained Binary Optimization (QUBO) [Kochenberger2014] is a model for representing a wide range of combinatorial optimization problems. Moreover, due to its close connection to Ising models, QUBO constitutes a central problem class for Adiabatic quantum computation feasible to be solved through quantum annealing.\n",
    "\n",
    "If $f_Q(x)=x^TQx$ is a quadratic polinomial over binary variables $x_i\\in B=\\{0,1\\}$, where $Q\\in \\mathbb {R} ^{n\\times n}$ is a symmetric $n\\times n$ matrix, the QUBO problem consists of finding a binary vector $x^{*}$ that minimize $f_Q$.\n",
    "\n",
    "\n",
    "## Goal\n",
    "<!--in a cuadratic form Vamos a armar código para dar como entrada\n",
    "el vector de los observables $\\alpha_i$ y los vectores que se usan para calcularlos a partir de la funcion de distribución($\\alpha_i=\\mathbf{\\alpha_i \\cdot P}$ y que nos devuelva la matriz $Q$ y el vector $C$ del problema binarizado QUBO ($P^TQP+C^TP$)-->\n",
    "\n",
    "We have redefined the MaxEnt problem as a QUBO problem $\\left(P^TQP+C^TP\\right)$. For this purpose, the first step is to find an appropriate entropy function $S$ and codified the variables to be obtained as a result of the minimization process, in our case the probabilities $p_i$, as a binary vector. Expanding the Shannon's entropy $H$ to first order in the distribution $P=(p_1,p_1,\\dots,p_k)^T$, we obtain the quadratic entropy\n",
    "\n",
    "\\begin{equation}\n",
    "H(P) \\approx −k\n",
    "\\sum_{i=1}^k p_i \\mathrm{log}(p_i)=2 - 2 P^TP,\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; (4)\\nonumber\n",
    "\\label{entropy}\n",
    "\\end{equation}\n",
    "\n",
    "were we have take the value $k=2$ according to a random variable $X\\in\\Omega=\\{0,1\\}$.\n",
    "Then, we are interesting in to find the probability distribution $P$ that satisfies the constraints in Eq. $\\left(1\\right)$ and maximizes the quadratic entropy $\\left(4\\right)$.\n",
    "\n",
    "$1.$  We define the cost function $f(P)$ as:\n",
    "\\begin{equation}\n",
    "f(P) = -H(P) + \\sum_{j= 0}^{m} (G_j^T P - \\alpha_j) ^2.\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; (5)\\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "where $G_j=(g_j(x_1),g_j(x_2),\\dots,g_j(x_k))^T$ is the vector which contains the image of the function $g_j\\in \\{g_1,\\dots,g_m\\}$. After trivial algebra manipulation Eq. $(5)$ is reduced to  \n",
    "\n",
    "\\begin{align}\n",
    "f(P)=-2 + \\sum_{j= 0}^{m}  \\alpha_j^2 +  \\left(\\sum_{j= 0}^{m}(-2) \\alpha_j G_j^T\\right) P + P^T \\left( 2I_k  +\\sum_{j= 0}^{m} G_j G_j^T\\right) P \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; (6)\\nonumber\n",
    " \\end{align}\n",
    "\n",
    "<!--\\begin{align}\n",
    "f(p) &=  -2 + 2p^{t}p + \\sum_{j= 0}^{m} (p^{t} y_j y_j^{t} p   -2 \\alpha_j y_j^{t} p + \\alpha_j^2) \\\\\n",
    " &= f(P)=\\sum_{j= 0}^{m}  \\alpha_j^2 -2 +  (\\sum_{j= 0}^{m}(-2) \\alpha_j y_j^{t}) p + p^{t} ( 2I_n  +\\sum_{j= 0}^{m} y_j y_j^{t}) p\n",
    " \\end{align}-->\n",
    "\n",
    "\n",
    "Then, $f(P)$ can be rewritten as\n",
    "\\begin{equation}\n",
    "f(P) =  P^T Q P + C^T P + cte, \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; (7)\\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "with $C^T = \\sum_{j= 0}^{m}(-2) \\alpha_j G_j^T$, and $Q = 2 I_n  +\\sum_{j= 0}^{m} G_j G_j^T$.\n",
    "\n",
    "$2.$ Now we will express each entry $p_i$ of the probability distribution in a binary basis, i.e., \n",
    "\\begin{equation}\n",
    "p_i  = \\sum_{k=1}^d \\frac{a_{ik}}{2^k},\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; (8)\\nonumber\n",
    "\\end{equation}\n",
    "where each $a_{ik}$ can be $0$ or $1$. For a given $d$ we have a restriction of the values that we able to represent $(0 \\leq p_i \\leq 1- 1/2^d$ with precision $1/2^d)$. Then,\n",
    "\n",
    "\\begin{align}\n",
    "p_i  &= (\\frac{1}{2}, \\ldots,\\frac{1}{2^d})  (a_{i1}, \\ldots, a_{id})^T\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; (9)\\nonumber\\\\\n",
    "\\mathrm{or}\\;\\; P & = S a,\\nonumber\n",
    "\\end{align}\n",
    "\n",
    "with $S$ an adequate matrix that performs the transformation.\n",
    "\n",
    "Finally, we obtain the cost function in a suitable form to be solve as a QUBO problem\n",
    "\n",
    "\\begin{equation}\n",
    "f(p) =  cte + C^T S a + a^T S^TQS a.\n",
    "\\end{equation}\n",
    "\n",
    "[Kochenberger2014] Kochenberger, Gary; Hao, Jin-Kao (2014). \"The unconstrained binary quadratic programming problem: a survey\" (PDF). Journal of Combinatorial Optimization. **28**: 58–81. http://doi:10.1007/s10878-014-9734-0. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b866891a",
   "metadata": {},
   "source": [
    "# A puzzlelike problem\n",
    "\n",
    "<a href=\"https://www.gifsanimados.org/cat-dados-710.htm\"><img src=\"https://www.gifsanimados.org/data/media/710/dado-imagen-animada-0094.gif\" border=\"0\" alt=\"dado-imagen-animada-0094\" /></a>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The following code finds the MaxEnt probability distribution given the appearance frequencies of the faces of a die as constraints. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ec6b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import function as f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d27c2082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary solution:  [0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0]\n",
      "Probability:  [0.15625  0.1875   0.1875   0.171875 0.140625 0.15625 ] Sum:  1.0\n",
      "Cost:  -0.9996630859375\n"
     ]
    }
   ],
   "source": [
    "#Example 1: Dice without constraints\n",
    "nb = 6             #Number of bits  \n",
    "lam = 0.001        #Optimization constant\n",
    "numreads = 10000   #number of reads\n",
    "y0= np.array ([[1.0],[1.0],[1.0],[1.0],[1.0], [1.0] ])\n",
    "alpha =  np.array ([1.0 ])\n",
    "y = [y0]\n",
    "x,p,cost, cost_bin = f.solution(y, alpha, nb, lam,numreads )\n",
    "print('Binary solution: ', x)\n",
    "print('Probability: ', p, 'Sum: ', sum(p))\n",
    "print('Cost: ', cost[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa89959",
   "metadata": {},
   "source": [
    "<img src=\"fig_Ex1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85d92008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary solution:  [0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 1 0]\n",
      "Probability:  [0.15625  0.171875 0.171875 0.171875 0.171875 0.15625 ] Sum:  1.0\n",
      "Cost:  -13.249666015625\n"
     ]
    }
   ],
   "source": [
    "#Example 2: Dice with fair mean value\n",
    "nb = 6             #Number of bits  \n",
    "lam = 0.001        #Optimization constant\n",
    "numreads = 10000   #number of reads\n",
    "y0= np.array ([[1.0],[1.0],[1.0],[1.0],[1.0], [1.0] ])\n",
    "y1= np.array ([[1.0],[2.0],[3.0],[4.0],[5.0], [6.0] ])\n",
    "alpha =  np.array ([1.0, 3.5 ])\n",
    "y = [y0, y1]\n",
    "x,p,cost, cost_bin = f.solution(y, alpha, nb, lam,numreads )\n",
    "print('Binary solution: ', x)\n",
    "print('Probability: ', p, 'Sum: ', sum(p))\n",
    "print('Cost: ', cost[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c464d9b",
   "metadata": {},
   "source": [
    "<img src=\"fig_Ex2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3629da51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary solution:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 1 0 1 1 1 1 1 0 1 0]\n",
      "Probability:  [0.        0.        0.        0.0234375 0.0078125 0.9765625] Sum:  1.0078125\n",
      "Cost:  -36.99968707275391\n"
     ]
    }
   ],
   "source": [
    "#Example 3: Loaded dice\n",
    "nb = 8             #Number of bits  \n",
    "lam = 0.0001       #Optimization constant\n",
    "numreads = 50000   #number of reads\n",
    "y0= np.array ([[1.0],[1.0],[1.0],[1.0],[1.0], [1.0] ])\n",
    "y1= np.array ([[1.0],[2.0],[3.0],[4.0],[5.0], [6.0] ])\n",
    "alpha =  np.array ([1.0, 6 ])\n",
    "y = [y0, y1]\n",
    "x,p,cost, cost_bin = f.solution(y, alpha, nb, lam,numreads )\n",
    "print('Binary solution: ', x)\n",
    "print('Probability: ', p, 'Sum: ', sum(p))\n",
    "print('Cost: ', cost[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ef0301",
   "metadata": {},
   "source": [
    "<img src=\"fig_Ex3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1076b01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary solution:  [0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1\n",
      " 0 1 0 0 0 0 0 1 1 0 0]\n",
      "Probability:  [0.046875   0.78515625 0.0390625  0.04296875 0.0390625  0.046875  ] Sum:  1.0\n",
      "Cost:  -1.6272644042968751\n"
     ]
    }
   ],
   "source": [
    "#Example 4: Dice with one face with fixed probability\n",
    "nb = 8             #Number of bits  \n",
    "lam = 0.01        #Optimization constant\n",
    "numreads = 10000   #number of reads\n",
    "y0= np.array ([[1.0],[1.0],[1.0],[1.0],[1.0], [1.0] ])\n",
    "y1= np.array ([[0.0],[1.0],[0.0],[0.0],[0.0], [0.0] ])\n",
    "alpha =  np.array ([1.0, 0.8 ])\n",
    "y = [y0, y1]\n",
    "x,p,cost, cost_bin = f.solution(y, alpha, nb, lam,numreads )\n",
    "print('Binary solution: ', x)\n",
    "print('Probability: ', p, 'Sum: ', sum(p))\n",
    "print('Cost: ', cost[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223680e8",
   "metadata": {},
   "source": [
    "<img src=\"fig_Ex4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1e23747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary solution:  [0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 0\n",
      " 1 0 0 0 0 0 1 0 1 0 0]\n",
      "Probability:  [0.34765625 0.34765625 0.07421875 0.07421875 0.078125   0.078125  ] Sum:  1.0\n",
      "Cost:  -1.4846789550781248\n"
     ]
    }
   ],
   "source": [
    "#Example 5: Dice with two faces summing a fixed probability\n",
    "nb = 8             #Number of bits  \n",
    "lam = 0.01        #Optimization constant\n",
    "numreads = 10000   #number of reads\n",
    "y0= np.array ([[1.0],[1.0],[1.0],[1.0],[1.0], [1.0] ])\n",
    "y1= np.array ([[1.0],[1.0],[0.0],[0.0],[0.0], [0.0] ])\n",
    "alpha =  np.array ([1.0, 0.7 ])\n",
    "y = [y0, y1]\n",
    "x,p,cost, cost_bin = f.solution(y, alpha, nb, lam,numreads )\n",
    "print('Binary solution: ', x)\n",
    "print('Probability: ', p, 'Sum: ', sum(p))\n",
    "print('Cost: ', cost[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d1ac4e",
   "metadata": {},
   "source": [
    "<img src=\"fig_Ex5.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
